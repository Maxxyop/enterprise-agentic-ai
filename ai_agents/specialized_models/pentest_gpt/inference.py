# Inference logic for the fine-tuned DeepSeek R1 model

from transformers import AutoModelForCausalLM, AutoTokenizer # type: ignore

class PentestGPTInference:
    def __init__(self, model_name: str):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

    def generate_response(self, prompt: str, max_length: int = 150) -> str:
        inputs = self.tokenizer.encode(prompt, return_tensors='pt')
        outputs = self.model.generate(inputs, max_length=max_length, num_return_sequences=1)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response

    def evaluate_prompt(self, prompt: str) -> str:
        # Placeholder for evaluation logic
        response = self.generate_response(prompt)
        return response

# Example usage
if __name__ == "__main__":
    model_name = "your-model-name-here"  # Replace with your model name
    pentest_gpt = PentestGPTInference(model_name)
    prompt = "What are the common vulnerabilities in web applications?"
    print(pentest_gpt.evaluate_prompt(prompt))